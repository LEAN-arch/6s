# six_sigma/data/session_state_manager.py
"""
Manages the application's session state, acting as an in-memory data source
for global manufacturing quality operations.

This module provides the SessionStateManager class, which initializes a rich,
interconnected, and realistic mock dataset reflecting the responsibilities of a
Quality Optimization Engineer Lead. The data model is generated by a separate,
dedicated function to simulate data from multiple sites, product lines, and
ongoing improvement initiatives.
"""

import logging
import random
from datetime import date, timedelta
from typing import Any, Dict, List, Optional
import streamlit as st
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

def _create_quality_optimizer_model(version: int) -> Dict[str, Any]:
    """
    Generates the complete, interconnected mock dataset for the Six Sigma Quality
    Command Center. This model simulates a multi-site manufacturing environment.
    """
    # --- Base Configuration ---
    np.random.seed(42)
    random.seed(42)
    base_date = date.today() - timedelta(days=365)
    sites = ["Eindhoven, NL", "Andover, US", "Shanghai, CN"]
    products_by_site = {
        "Eindhoven, NL": ["IntelliVue Patient Monitor", "Zenition C-arm"],
        "Andover, US": ["IntelliVue Patient Monitor", "HeartStart Defibrillator"],
        "Shanghai, CN": ["Zenition C-arm", "Affiniti Ultrasound"]
    }

    # --- 1. Generate Historical KPI Data ---
    kpi_data = []
    for i in range(365):
        current_date = base_date + timedelta(days=i)
        for site in sites:
            for product in products_by_site[site]:
                # Simulate a gradual improvement trend over the year
                improvement_factor = (1 - (i / 730))
                copq_base = 50000 if "Monitor" in product else 75000
                ftr_base = 0.92 if "NL" in site else 0.88

                kpi_data.append({
                    "date": current_date,
                    "site": site,
                    "product_line": product,
                    "copq": int(copq_base * improvement_factor * (1 + np.random.uniform(-0.1, 0.1))),
                    "ftr_rate": ftr_base * improvement_factor * (1 + np.random.uniform(-0.02, 0.02)),
                    "scrap_rate": (1 - (ftr_base * improvement_factor)) * (1 + np.random.uniform(-0.1, 0.1))
                })

    # --- 2. Generate Process Data for a Specific Product ---
    # For DMAIC analysis (e.g., Process Capability)
    process_data = {
        'timestamp': pd.to_datetime(pd.date_range(start=base_date, periods=200, freq='D')),
        'voltage_a': np.random.normal(5.0, 0.05, 200),
        'pressure_b': np.random.normal(100, 2.5, 200),
        'seal_temperature': np.random.normal(150, 1.2, 200)
    }
    # Introduce a process shift for realism
    process_data['voltage_a'][100:] += 0.1
    process_data_df = pd.DataFrame(process_data)
    
    # --- 3. Define DMAIC Projects ---
    dmaic_projects = [
        {
            "id": "DMAIC-001", "site": "Andover, US", "product_line": "HeartStart Defibrillator",
            "title": "Reduce Capacitor Charging Failures",
            "phase": "Analyze",
            "problem_statement": "The final test failure rate for the capacitor charging module is 12%, significantly above the 4% target, leading to high scrap costs and rework.",
            "goal_statement": "Reduce the capacitor charging failure rate from 12% to less than 4% by Q4, resulting in an estimated annual COPQ saving of $250k.",
            "team": ["John Smith (Lead)", "Jane Doe (Engineer)", "Mike Ross (Ops)"],
            "start_date": base_date + timedelta(days=180),
        },
        {
            "id": "DMAIC-002", "site": "Eindhoven, NL", "product_line": "Zenition C-arm",
            "title": "Improve X-Ray Tube Alignment Yield",
            "phase": "Improve",
            "problem_statement": "The FTR for the X-ray tube alignment process is 85%, requiring significant rework and calibration time, impacting overall cycle time.",
            "goal_statement": "Increase the FTR for X-ray tube alignment to >95% by year-end, reducing cycle time by 8 hours per unit.",
            "team": ["Elena Reyes, PhD (Lead)", "Ben Carter, MD (SME)"],
            "start_date": base_date + timedelta(days=90),
        },
        {
            "id": "DMAIC-003", "site": "Shanghai, CN", "product_line": "Affiniti Ultrasound",
            "title": "Optimize Transducer Crystal Bonding Process",
            "phase": "Control",
            "problem_statement": "The transducer crystal bonding process exhibits high variability, leading to acoustic artifacts identified in 2% of units during final QC.",
            "goal_statement": "Implement process controls to reduce acoustic artifact defects to <0.5%, improving product reliability and reducing warranty claims.",
            "team": ["Li Wei (Lead)", "Chen Yue (Engineer)"],
            "start_date": base_date + timedelta(days=30),
        }
    ]

    # --- 4. Generate Product Release & Sampling Data ---
    release_batches = []
    for i in range(50):
        batch_id = f"BATCH-{202400 + i}"
        is_fail = random.random() < 0.05 # 5% baseline failure rate
        measurement = np.random.normal(10.5, 0.8) if is_fail else np.random.normal(10.0, 0.2)
        release_batches.append({
            "batch_id": batch_id,
            "product_line": "IntelliVue Patient Monitor",
            "site": "Eindhoven, NL",
            "lot_size": 1000,
            "test_measurement": measurement,
            "true_status": "Fail" if is_fail else "Pass"
        })

    # --- 5. Generate Kaizen & Training Data ---
    kaizen_events = [
        {"id": "KZN-01", "site": "Eindhoven, NL", "date": base_date + timedelta(days=250), "title": "5S Implementation on Monitor Assembly Line 3", "outcome": "Reduced tool search time by 60%; cleared 25 sq. meters of floor space."},
        {"id": "KZN-02", "site": "Andover, US", "date": base_date + timedelta(days=300), "title": "Value Stream Mapping of Defibrillator Sub-assembly", "outcome": "Identified 3 non-value-add steps, reducing process cycle time by 15%."}
    ]
    training_materials = [
        {"id": "TRN-001", "title": "Introduction to DMAIC Methodology", "type": "eLearning", "duration_hr": 4, "link": "#"},
        {"id": "TRN-002", "title": "Statistical Process Control (SPC) Fundamentals", "type": "PDF Guide", "duration_hr": 2, "link": "#"},
        {"id": "TRN-003", "title": "Root Cause Analysis & 5 Whys", "type": "Workshop Slides", "duration_hr": 3, "link": "#"},
    ]
    
    # --- 6. Generate Predictive Quality Data ---
    predictive_data = []
    for _ in range(500):
        # Simulate a process where higher temp and pressure lead to failure
        temp = np.random.normal(200, 10)
        pressure = np.random.normal(50, 5)
        vibration = np.random.normal(1.5, 0.5)
        # Failure probability increases with temp and pressure
        fail_prob = 1 / (1 + np.exp(-(0.1*(temp-210) + 0.5*(pressure-52))))
        outcome = "Fail" if random.random() < fail_prob else "Pass"
        predictive_data.append({
            "in_process_temp": temp,
            "in_process_pressure": pressure,
            "in_process_vibration": vibration,
            "final_qc_outcome": outcome
        })

    # --- Assemble the Final Data Model ---
    return {
        "data_version": version,
        "global_kpis": kpi_data,
        "process_data": {
            "source_product": "IntelliVue Patient Monitor",
            "source_site": "Eindhoven, NL",
            "specs": {
                "voltage_a": {"lsl": 4.9, "usl": 5.1},
                "pressure_b": {"lsl": 95, "usl": 105},
                "seal_temperature": {"lsl": 147, "usl": 153}
            },
            "data": process_data_df
        },
        "dmaic_projects": dmaic_projects,
        "release_data": release_batches,
        "kaizen_events": kaizen_events,
        "training_materials": training_materials,
        "predictive_quality_data": pd.DataFrame(predictive_data)
    }

class SessionStateManager:
    """Handles the initialization and access of the application's session state."""
    _DATA_KEY = "six_sigma_quality_data"
    _CURRENT_DATA_VERSION = 1

    def __init__(self):
        """Initializes the session state, loading the mock data if necessary."""
        session_data = st.session_state.get(self._DATA_KEY)
        if not session_data or session_data.get("data_version") != self._CURRENT_DATA_VERSION:
            logger.info(f"Initializing session state with Quality Optimizer data model v{self._CURRENT_DATA_VERSION}.")
            try:
                st.session_state[self._DATA_KEY] = _create_quality_optimizer_model(self._CURRENT_DATA_VERSION)
                logger.info("Session state initialized successfully.")
            except Exception as e:
                logger.critical(f"FATAL: Data generation failed: {e}", exc_info=True)
                st.error(f"A critical error occurred during application startup: {e}. The application cannot continue.", icon="ðŸš¨")
                st.stop()

    def get_data(self, primary_key: str, secondary_key: Optional[str] = None) -> Any:
        """
        Safely retrieves data from the nested session state dictionary.
        Returns an empty dict or list if the key is not found.
        """
        try:
            data_store = st.session_state.get(self._DATA_KEY, {})
            if secondary_key:
                return data_store.get(primary_key, {}).get(secondary_key, [])
            else:
                return data_store.get(primary_key, {})
        except (KeyError, AttributeError):
            logger.warning(f"Attempted to access non-existent or malformed session state key: '{primary_key}/{secondary_key}'")
            return {} if secondary_key is None else []

    def update_data(self, data: Any, primary_key: str, secondary_key: Optional[str] = None) -> None:
        """
        Updates the session state with new data.
        """
        try:
            if self._DATA_KEY not in st.session_state:
                st.session_state[self._DATA_KEY] = {}

            if secondary_key:
                if primary_key not in st.session_state[self._DATA_KEY]:
                    st.session_state[self._DATA_KEY][primary_key] = {}
                st.session_state[self._DATA_KEY][primary_key][secondary_key] = data
            else:
                st.session_state[self._DATA_KEY][primary_key] = data
            logger.info(f"Session state updated for {primary_key}.{secondary_key if secondary_key else ''}")
        except Exception as e:
            logger.error(f"Failed to update session state for {primary_key}/{secondary_key}: {e}", exc_info=True)
            st.error("A critical error occurred while trying to save data. Please refresh the page.")
